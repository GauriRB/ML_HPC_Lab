{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauriRB/ML_HPC_Lab/blob/main/ParallelKNN_ondisease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "yAKVauJtNypM",
        "outputId": "868ddfb4-1d8b-4a20-de6e-367ce694ba0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4753387421fd>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-4753387421fd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m#returns a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Replace df with normalized values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '\\t?'"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "from itertools import repeat\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "\n",
        "\n",
        "#for plotting\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "class CustomKNN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.accurate_predictions = 0\n",
        "        self.total_predictions = 0\n",
        "        self.accuracy = 0.0\n",
        "\n",
        "    def predict(self, training_data, to_predict, k = 3):\n",
        "        if len(training_data) >= k:\n",
        "            print(\"K cannot be smaller than the total voting groups(ie. number of training data points)\")\n",
        "            return\n",
        "\n",
        "        distributions = []\n",
        "        for group in training_data:\n",
        "            for features in training_data[group]:\n",
        "                # Find euclidean distance using the numpy function\n",
        "                euclidean_distance = np.linalg.norm(np.array(features)- np.array(to_predict))\n",
        "                distributions.append([euclidean_distance, group])\n",
        "        # Find the k nearest neighbors\n",
        "        results = [i[1] for i in sorted(distributions)[:k]]\n",
        "        # Figure out which is the most common class amongst the neighbors.\n",
        "        result = Counter(results).most_common(1)[0][0]\n",
        "        confidence = Counter(results).most_common(1)[0][1]/k\n",
        "\n",
        "        return result, to_predict\n",
        "\n",
        "    def test(self, test_set, training_set):\n",
        "        pool = mp.Pool(processes= 8)\n",
        "\n",
        "        arr = {}\n",
        "        s = time.perf_counter()\n",
        "\n",
        "        # Where the magic happens, this is where we parallelize our code. While testing for the classes of incoming points,\n",
        "        # we divide the incoming data points and feed them into the predict funtion in parallel.\n",
        "        # I have used the starpmap function of multiprocessing library for this purpose.\n",
        "        # The training data gets repeated to get an iterable of the training dataset for the map function, ie. the predict funtion, to be applied on.\n",
        "        for group in test_set:\n",
        "            arr[group] =  pool.starmap(self.predict, zip(repeat(training_set), test_set[group], repeat(3)))\n",
        "        e = time.perf_counter()\n",
        "\n",
        "        #Calculating Accuracy - The accuracy code has to be modified due to the induced parallelism.\n",
        "        # It is no longer possible to determinstically calculate the accurate predictions where multiple subprocesses are doing the same increment.\n",
        "\n",
        "        for group in test_set:\n",
        "            for data in test_set[group]:\n",
        "                for i in arr[group]:\n",
        "                    if data == i[1]:\n",
        "                        self.total_predictions += 1\n",
        "                        if group == i[0]:\n",
        "                            self.accurate_predictions+=1\n",
        "\n",
        "        self.accuracy = 100*(self.accurate_predictions/self.total_predictions)\n",
        "        print(\"\\nAcurracy :\", str(self.accuracy) + \"%\")\n",
        "\n",
        "def mod_data(df):\n",
        "    df.replace('?', -999999, inplace = True)\n",
        "\n",
        "    df.replace('yes', 4, inplace = True)\n",
        "    df.replace('no', 2, inplace = True)\n",
        "\n",
        "    df.replace('notpresent', 4, inplace = True)\n",
        "    df.replace('present', 2, inplace = True)\n",
        "\n",
        "    df.replace('abnormal', 4, inplace = True)\n",
        "    df.replace('normal', 2, inplace = True)\n",
        "\n",
        "    df.replace('poor', 4, inplace = True)\n",
        "    df.replace('good', 2, inplace = True)\n",
        "\n",
        "    df.replace('ckd', 4, inplace = True)\n",
        "    df.replace('notckd', 2, inplace = True)\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(r\"ChronicKidneyDisease.csv\")\n",
        "    mod_data(df)\n",
        "  #  dataset = df.astype(float).values.tolist()\n",
        "\n",
        "    #Normalize the data\n",
        "    x = df.values #returns a numpy array\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled) #Replace df with normalized values\n",
        "\n",
        "    #Shuffle the dataset\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    #20% of the available data will be used for testing\n",
        "    test_size = 0.1\n",
        "\n",
        "    #The keys of the dict are the classes that the data is classfied into\n",
        "    training_set = {2: [], 4:[]}\n",
        "    test_set = {2: [], 4:[]}\n",
        "\n",
        "    #Split data into training and test for cross validation\n",
        "    training_data = dataset[:-int(test_size * len(dataset))]\n",
        "    test_data = dataset[-int(test_size * len(dataset)):]\n",
        "\n",
        "    #Insert data into the training set\n",
        "    for record in training_data:\n",
        "        training_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\n",
        "    #Insert data into the test set\n",
        "    for record in test_data:\n",
        "        test_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\n",
        "    s = time.perf_counter()\n",
        "    knn = CustomKNN()\n",
        "    knn.test(test_set, training_set)\n",
        "    e = time.perf_counter()\n",
        "\n",
        "    print(\"Exec Time: \", e-s)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "from itertools import repeat\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "\n",
        "\n",
        "#for plotting\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "class CustomKNN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.accurate_predictions = 0\n",
        "        self.total_predictions = 0\n",
        "        self.accuracy = 0.0\n",
        "\n",
        "    def predict(self, training_data, to_predict, k = 3):\n",
        "        if len(training_data) >= k:\n",
        "            print(\"K cannot be smaller than the total voting groups(ie. number of training data points)\")\n",
        "            return\n",
        "\n",
        "        distributions = []\n",
        "        for group in training_data:\n",
        "            for features in training_data[group]:\n",
        "                # Find euclidean distance using the numpy function\n",
        "                euclidean_distance = np.linalg.norm(np.array(features)- np.array(to_predict))\n",
        "                distributions.append([euclidean_distance, group])\n",
        "        # Find the k nearest neighbors\n",
        "        results = [i[1] for i in sorted(distributions)[:k]]\n",
        "        # Figure out which is the most common class amongst the neighbors.\n",
        "        result = Counter(results).most_common(1)[0][0]\n",
        "        confidence = Counter(results).most_common(1)[0][1]/k\n",
        "\n",
        "        return result, to_predict\n",
        "\n",
        "    def test(self, test_set, training_set):\n",
        "        pool = mp.Pool(processes= 8)\n",
        "\n",
        "        arr = {}\n",
        "        s = time.process_time()\n",
        "\n",
        "        # Where the magic happens, this is where we parallelize our code. While testing for the classes of incoming points,\n",
        "        # we divide the incoming data points and feed them into the predict funtion in parallel.\n",
        "        # I have used the starpmap function of multiprocessing library for this purpose.\n",
        "        # The training data gets repeated to get an iterable of the training dataset for the map function, ie. the predict funtion, to be applied on.\n",
        "        for group in test_set:\n",
        "            arr[group] =  pool.starmap(self.predict, zip(repeat(training_set), test_set[group], repeat(3)))\n",
        "        e = time.process_time()\n",
        "\n",
        "        #Calculating Accuracy - The accuracy code has to be modified due to the induced parallelism.\n",
        "        # It is no longer possible to determinstically calculate the accurate predictions where multiple subprocesses are doing the same increment.\n",
        "\n",
        "        for group in test_set:\n",
        "            for data in test_set[group]:\n",
        "                for i in arr[group]:\n",
        "                    if data == i[1]:\n",
        "                        self.total_predictions += 1\n",
        "                        if group == i[0]:\n",
        "                            self.accurate_predictions+=1\n",
        "\n",
        "        self.accuracy = 100*(self.accurate_predictions/self.total_predictions)\n",
        "        print(\"\\nAcurracy :\", str(self.accuracy) + \"%\")\n",
        "\n",
        "def mod_data(df):\n",
        "    df.replace('?', -999999, inplace = True)\n",
        "\n",
        "    df.replace('yes', 4, inplace = True)\n",
        "    df.replace('no', 2, inplace = True)\n",
        "\n",
        "    df.replace('notpresent', 4, inplace = True)\n",
        "    df.replace('present', 2, inplace = True)\n",
        "\n",
        "    df.replace('abnormal', 4, inplace = True)\n",
        "    df.replace('normal', 2, inplace = True)\n",
        "\n",
        "    df.replace('poor', 4, inplace = True)\n",
        "    df.replace('good', 2, inplace = True)\n",
        "\n",
        "    df.replace('ckd', 4, inplace = True)\n",
        "    df.replace('notckd', 2, inplace = True)\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(r\"chronic_kidney_disease.csv\")\n",
        "    mod_data(df)\n",
        "    dataset = df.astype(float).values.tolist()\n",
        "\n",
        "    #Normalize the data\n",
        "    x = df.values #returns a numpy array\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled) #Replace df with normalized values\n",
        "\n",
        "    #Shuffle the dataset\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    #20% of the available data will be used for testing\n",
        "    test_size = 0.1\n",
        "\n",
        "    #The keys of the dict are the classes that the data is classfied into\n",
        "    training_set = {2: [], 4:[]}\n",
        "    test_set = {2: [], 4:[]}\n",
        "\n",
        "    #Split data into training and test for cross validation\n",
        "    training_data = dataset[:-int(test_size * len(dataset))]\n",
        "    test_data = dataset[-int(test_size * len(dataset)):]\n",
        "\n",
        "    #Insert data into the training set\n",
        "    for record in training_data:\n",
        "        training_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\n",
        "    #Insert data into the test set\n",
        "    for record in test_data:\n",
        "        test_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\n",
        "    s = time.process_time()\n",
        "    knn = CustomKNN()\n",
        "    knn.test(test_set, training_set)\n",
        "    e = time.process_time()\n",
        "\n",
        "    print(\"Exec Time: \", e-s)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Rag2ZXfjRoEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsh06J9CRqHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
